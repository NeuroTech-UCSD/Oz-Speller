{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainda.datasets import Nakanishi2015, Wang2016\n",
    "from brainda.paradigms import SSVEP\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Nakanishi2015()\n",
    "dataset = Wang2016()\n",
    "events = sorted(list(dataset.events.keys()))\n",
    "freqs = [dataset.get_freq(event) for event in events]\n",
    "phases = [dataset.get_phase(event) for event in events]\n",
    "start_pnt = dataset.events[events[0]][1][0]\n",
    "delay = 0.14 # seconds\n",
    "channels = ['PZ', 'PO5', 'PO3', 'POZ', 'PO4', 'PO6', 'O1', 'OZ', 'O2']\n",
    "srate = 250 # Hz\n",
    "duration = 0.5 # seconds\n",
    "paradigm = SSVEP(\n",
    "    srate=srate, \n",
    "    channels=channels, \n",
    "    intervals=[(start_pnt+delay, start_pnt+delay+duration+0.1)], # more seconds for TDCA \n",
    "    events=events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 40, 9, 150)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y, meta = paradigm.get_data(\n",
    "    dataset, \n",
    "    subjects=[1], \n",
    "    return_concat=False, \n",
    "    n_jobs=1, \n",
    "    verbose=False)\n",
    "eeg = np.zeros((40, 6, 9, 150))\n",
    "y_temp = np.zeros((40, 6))\n",
    "for i_target, target in enumerate(X.keys()):\n",
    "    eeg[i_target] = X[target]\n",
    "eeg = eeg.transpose((1,0,2,3))\n",
    "X = eeg[...,np.newaxis]\n",
    "y = y_temp.T\n",
    "eeg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 40, 9, 150, 1), (6, 40), 40, 40)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape,len(freqs),len(phases) # 6 trials x 40 classes\n",
    "# (240, 9, 150), (240,) --> (6, 40, 9, 150, 1), (6, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EEGNet_SSVEP(nb_classes, Chans, Samples, \n",
    "             dropoutRate = 0.3, kernLength = 256, F1 = 96, \n",
    "             D = 1, F2 = 96, dropoutType = 'Dropout'):\n",
    "    \"\"\" SSVEP Variant of EEGNet, as used in [1]. \n",
    "    Inputs:\n",
    "        \n",
    "      nb_classes      : int, number of classes to classify\n",
    "      Chans, Samples  : number of channels and time points in the EEG data\n",
    "      dropoutRate     : dropout fraction\n",
    "      kernLength      : length of temporal convolution in first layer\n",
    "      F1, F2          : number of temporal filters (F1) and number of pointwise\n",
    "                        filters (F2) to learn. \n",
    "      D               : number of spatial filters to learn within each temporal\n",
    "                        convolution.\n",
    "      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n",
    "      \n",
    "      \n",
    "    [1]. Waytowich, N. et. al. (2018). Compact Convolutional Neural Networks\n",
    "    for Classification of Asynchronous Steady-State Visual Evoked Potentials.\n",
    "    Journal of Neural Engineering vol. 15(6). \n",
    "    http://iopscience.iop.org/article/10.1088/1741-2552/aae5d8\n",
    "    \"\"\"\n",
    "    \n",
    "    if dropoutType == 'SpatialDropout2D':\n",
    "        dropoutType = tf.keras.layers.SpatialDropout2D\n",
    "    elif dropoutType == 'Dropout':\n",
    "        dropoutType = tf.keras.layers.Dropout\n",
    "    else:\n",
    "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                         'or Dropout, passed as a string.')\n",
    "    \n",
    "    input1   = tf.keras.Input(shape = (Chans, Samples, 1))\n",
    "\n",
    "    ##################################################################\n",
    "    block1       = tf.keras.layers.Conv2D(F1, (1, kernLength), padding = 'same',\n",
    "                                   input_shape = (Chans, Samples, 1),\n",
    "                                   use_bias = False)(input1)\n",
    "    block1       = tf.keras.layers.BatchNormalization()(block1)\n",
    "    block1       = tf.keras.layers.DepthwiseConv2D((Chans, 1), use_bias = False, \n",
    "                                   depth_multiplier = D,\n",
    "                                   depthwise_constraint = tf.keras.constraints.max_norm(1.))(block1)\n",
    "    block1       = tf.keras.layers.BatchNormalization()(block1)\n",
    "    block1       = tf.keras.layers.Activation('elu')(block1)\n",
    "    block1       = tf.keras.layers.AveragePooling2D((1, 4))(block1)\n",
    "    block1       = dropoutType(dropoutRate)(block1)\n",
    "    \n",
    "    block2       = tf.keras.layers.SeparableConv2D(F2, (1, 16),\n",
    "                                   use_bias = False, padding = 'same')(block1)\n",
    "    block2       = tf.keras.layers.BatchNormalization()(block2)\n",
    "    block2       = tf.keras.layers.Activation('elu')(block2)\n",
    "    block2       = tf.keras.layers.AveragePooling2D((1, 8))(block2)\n",
    "    block2       = dropoutType(dropoutRate)(block2)\n",
    "        \n",
    "    flatten      = tf.keras.layers.Flatten(name = 'flatten')(block2)\n",
    "    \n",
    "    dense        = tf.keras.layers.Dense(nb_classes, name = 'dense')(flatten)\n",
    "    softmax      = tf.keras.layers.Activation('softmax', name = 'softmax')(dense)\n",
    "    \n",
    "    return tf.keras.Model(inputs=input1, outputs=softmax)\n",
    "num_channels = 9\n",
    "timepoints_stimulus_duration = 150\n",
    "# hyperparameters\n",
    "epochs = 150\n",
    "batch_size = 100\n",
    "learning_rate = 1e-3\n",
    "dropout_rate = 0.5\n",
    "# build model\n",
    "kf = KFold(n_splits=6)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index].reshape(-1, *X[train_index].shape[2:]), X[test_index].reshape(-1, *X[test_index].shape[2:])\n",
    "    y_train, y_test = y[train_index].reshape(-1), y[test_index].reshape(-1)\n",
    "    print(y_test[:5], y_train[:5])\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_test.shape, y_test.shape)\n",
    "    model = EEGNet_SSVEP(len(freqs), num_channels, timepoints_stimulus_duration, dropoutRate=dropout_rate)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f95cac2866d48a154725f15da66ad3889a01053732bcd1fedad154961939282"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
